
Input:
You are an accomplished researcher in machine learning. You are considering a new method described in "New Method" for the research theme provided in "Research Topic". "Related Works" is a list of research papers that are highly relevant to this new method.
Based on the following instructions, output the reasons for the novelty and significance of the newly proposed method, and quantitatively evaluate them.

# Research Topic
LLMの新しい損失関数

# New Method
{
    "Open Problems": "Brier / Spherical losses give better calibration and sometimes better generation quality when used for fine-tuning, but: (1) training from scratch converges slowly and final perplexity is worse than with log-likelihood, because the bounded gradient of Brier gives weak early learning signals; (2) pure log-likelihood converges fast but produces over-confident, poorly calibrated models. A minimal change that blends the strong early gradient of log-likelihood with the good late-stage calibration of Brier is still missing.",
    "Methods": "Hybrid Log–Brier (HLB) loss: L_HLB = α(t) · CE + (1−α(t)) · Brier.\n• CE is the standard token-level cross-entropy (negative log-likelihood).\n• Brier is the token-level squared error between predicted probability and one-hot target.\n• α(t) is a simple deterministic schedule over training steps t.  A linear decay works well: α(t)=max(0,1−t/T_switch).  This keeps the loss identical to CE at the start and identical to Brier after T_switch steps.\nTheoretical intuition: in early training, model predictions are close to uniform, so CE’s unbounded gradient accelerates learning.  As predictions sharpen, α(t)→0 limits gradient magnitude and encourages calibrated probabilities.  Because both terms are strictly proper, the mixture remains strictly proper for any α∈(0,1).",
    "Experimental Setup": "Tiny experiment that fits on a laptop GPU.\nModel: DistilGPT-2 (82 M params).\nDataset: WikiText-2 language-modeling split.\nTraining: 3 epochs, AdamW, LR 5e-4.\nBaselines: (1) CE only, (2) Brier only.  Proposed: HLB with T_switch = 50 % of total steps.\nMetrics:\n• Perplexity (lower is better) – evaluates fit.\n• Expected Calibration Error (ECE, bin=10) – evaluates probability calibration.\n• Negative Log-Likelihood (NLL) on validation set reported for completeness.",
    "Experimental Code": "import torch, torch.nn.functional as F\n\ndef hlb_loss(logits, targets, step, total_steps, t_switch_ratio=0.5):\n    \"\"\"Hybrid Log–Brier loss with linear α schedule.\"\"\"\n    probs = F.softmax(logits, dim=-1)\n    log_probs = F.log_softmax(logits, dim=-1)\n    one_hot = F.one_hot(targets, num_classes=logits.size(-1)).float()\n\n    ce = -(log_probs * one_hot).sum(-1)          # token-wise CE\n    brier = ((probs - one_hot) ** 2).sum(-1)     # token-wise Brier\n\n    T_switch = total_steps * t_switch_ratio\n    alpha = max(0.0, 1.0 - step / T_switch) if step < T_switch else 0.0\n\n    return (alpha * ce + (1 - alpha) * brier).mean()\n\n# Example usage inside a training loop\n# for step, batch in enumerate(loader):\n#     logits = model(input_ids=batch[\"input_ids\"]).logits\n#     loss = hlb_loss(logits, batch[\"labels\"], step, total_steps)\n#     loss.backward(); optimizer.step()\n",
    "Expected Result": "• CE baseline: lowest perplexity (~35) but high ECE (~0.12).\n• Brier baseline: highest perplexity (~46) but low ECE (~0.04).\n• HLB: perplexity close to CE (≈37) while keeping calibration close to Brier (ECE ≈0.05).\nThus HLB maintains most of the likelihood fit of CE and almost all the calibration gain of Brier.",
    "Expected Conclusion": "A single line change that linearly blends CE with Brier during training yields models that learn as fast as CE early on but inherit the calibrated probabilities of Brier at convergence.  Because both terms are simple and differentiable, the method drops into any existing training code base with minimal effort and no additional hyper-parameters beyond an optional switch point.  This demonstrates that small, principled tweaks to the loss can simultaneously improve convergence and reliability in LLM training."
}

# Related Works
{
    "Title": "Discovering Preference Optimization Algorithms with and for Large Language Models",
    "Main Contributions": "The paper introduces an LLM-driven objective discovery pipeline to automatically identify new state-of-the-art offline preference optimization algorithms for Large Language Models (LLMs), addressing the limitations of human creativity in designing loss functions. It proposes Discovered Preference Optimization (DiscoPOP), a novel algorithm that adaptively blends logistic and exponential losses. DiscoPOP demonstrates state-of-the-art performance and successful transferability across held-out tasks, and its analysis reveals surprising features like non-convexity and negative gradients at the starting point.",
    "Methodology": "The core methodology is an LLM-driven objective discovery pipeline. It iteratively prompts a large language model (GPT-4) to propose and implement new Python code for preference optimization loss functions. The LLM is initialized with established loss functions and their performance, then receives feedback on the performance (MT-Bench scores) of its proposed functions to refine its generation strategy. Each proposed function undergoes unit tests, and if valid, an LLM is fine-tuned and evaluated. The best-performing discovered algorithm, DiscoPOP (Log Ratio Modulated Loss, LRML), dynamically combines logistic and exponential losses, with weights determined by a sigmoid calculation of the difference in log-ratios (flrml(βρ) = (1− σ(βρ/τ )) · fdpo(βρ) +σ(βρ/τ ) · fexp(−βρ)).",
    "Experimental Setup": "The discovery process focused on multi-turn dialogue on MT-Bench using 'zephyr-7b-gemma-sft' (Gemma-7B fine-tuned on 'deita-10k-v0-sft') and the 'Argilla DPO Mix 7K' preference dataset. Models were trained for two epochs with a fixed β=0.05, learning rate 5e-7, and AdamW optimizer on 8 Nvidia A100 GPUs. Held-out evaluations included: 1) Single-turn dialogue on Alpaca Eval 2.0 (win rate against GPT-4 or SFT base model), 2) Summarization (TL;DR dataset, 10% subsample, evaluated using Alpaca Eval 2.0 with custom GPT-4 annotator), and 3) Positive sentiment generation (IMDb dataset, fine-tuning GPT-2, evaluated by a pre-trained sentiment classifier and KL-Divergence across various β values). A small case study on CIFAR-10 classification with ResNet-18 was also performed to demonstrate general objective discovery.",
    "Limitations": "The current approach has several limitations: the LLM objective proposal generation methods are still nascent, with initial exploratory techniques (e.g., temperature sampling, worst-to-best sorting) not yielding significant improvements. The `β` parameter in DiscoPOP was fixed during discovery (β=0.05), leading to convergence issues when β is too low (≤ 0.01) or too high (≥ 2.5), potentially due to large gradients in its non-convex regions. Furthermore, the reliance on closed-source models (GPT-4) for code generation limits reproducibility and incurs high costs.",
    "Future Research Directions": "Future research could explore more effective LLM objective proposal generation techniques, such as leveraging Visual Language Models with learning curve plots or meta-meta-optimizing the LLM prompt. A multi-parameter analysis of DiscoPOP, where multiple floating-point parameters are tuned separately, is suggested to address the limitations observed with the fixed β parameter during discovery. The paper also proposes using the produced models themselves to generate code, leading to code-level self-improvement, and enforcing constraints on the exploring LLM to ensure consistent handling of parameters like β within the loss functions.",
    "Experiment Code": "The core of the experimental code for the LLM-driven objective discovery pipeline includes:\n\n1.  **LLM prompting system prompt (scripts/launch_evo.py):**\n    ```python\n    system_prompt = \"\"\"\nYou are a machine learning researcher who is testing out different RLHF loss functions. When you respond, output a JSON where the first key (\"thought\") corresponds to your thought process when designing the next function. The second key (\"name\") corresponds to the name of your next function. Finally, the last key (\"code\") corresponds to the exact python code that you would like to try. Here is an example:\n\n{\"thought\": \"Based on the previous outputs, I should try the direct preference optimization algorithm.\",\n\"name\": \"dpo\",\n\"code\": \"def sigmoid_loss(\n    self,\n    policy_chosen_logps: torch.FloatTensor,\n    policy_rejected_logps: torch.FloatTensor,\n    reference_chosen_logps: torch.FloatTensor,\n    reference_rejected_logps: torch.FloatTensor,\n) -> torch.FloatTensor:\n    pi_logratios = policy_chosen_logps - policy_rejected_logps\n    ref_logratios = reference_chosen_logps - reference_rejected_logps\n    logits = pi_logratios - ref_logratios\n    losses = -F.logsigmoid(self.beta * logits)\n    return losses\"\n}\n\nYou are deeply familiar with binary classification losses from the literature. Be creative and reference prior literature when possible.\n\nYou must use the exact function interface used above. Feel free to define extra hyperparameters within your function as constants. Do not make them attributes of self.\n\nNote that `self.beta = 0.05`.\n\nRLHF loss functions train on a dataset of pairs of preferred and rejected completions.\n`policy_chosen_logps` refers to the policy's log probabilities of the preferred completion, and `policy_rejected_logps` refers to the policy's log probabilities of the rejected completion.\n`reference_chosen_logps` and `reference_rejected_logps` refer to the same for the reference (base) model.\n\nThe user will then return to you a fitness that corresponds to the performance of the resulting model on a downstream task. Your goal is to maximize performance.\n\"\"\"\n    ```\n\n2.  **Code validation function (scripts/launch_evo.py):**\n    ```python\n    def validate_code(code: str) -> bool:\n        # Run code through test\n        try:\n            # Namespace dictionary to hold the execution context\n            namespace = {}\n\n            # Execute the function definition string within the provided namespace\n            exec(code, globals(), namespace)\n\n            names = list(namespace.keys())\n            if len(names) != 1:\n                return False, f\"{len(names)} things in namespace. Please only provide 1\"\n            func = namespace[names[0]]\n            if not callable(func):\n                return False, f\"{func} is not callable\"\n\n            # Create a class to hold the sigmoid_loss function\n            class LossModel:\n                def __init__(self, beta):\n                    self.beta = beta\n\n            # Add the function to the class\n            setattr(LossModel, \"epo_loss\", func)\n\n            model = LossModel(beta=0.05)\n\n            # Define input tensors with requires_grad to check gradients\n            policy_chosen_logps = torch.randn(10, requires_grad=True)\n            policy_rejected_logps = torch.randn(10, requires_grad=True)\n            reference_chosen_logps = torch.randn(10, requires_grad=True)\n            reference_rejected_logps = torch.randn(10, requires_grad=True)\n\n            # Compute the loss\n            loss = model.epo_loss(\n                policy_chosen_logps,\n                policy_rejected_logps,\n                reference_chosen_logps,\n                reference_rejected_logps,\n            )\n\n            # Check for NaNs in the output\n            if torch.isnan(loss).any():\n                return False, \"Loss contains NaNs\"\n\n            # Check the shape of the output\n            if loss.shape != (10,):\n                return (\n                    False,\n                    f\"Expected loss shape to be per input (e.g. (10,)), got {loss.shape}\",\n                )\n\n            # Backward pass to compute gradients\n            loss.mean().backward()\n\n            # Check for NaNs in gradients\n            for param in [\n                policy_chosen_logps,\n                policy_rejected_logps,\n                reference_chosen_logps,\n                reference_rejected_logps,\n            ]:\n                if torch.isnan(param.grad).any():\n                    return False, f\"Gradient for {param} contains NaNs\"\n\n            return True, \"\"\n\n        except Exception as e:\n            return False, str(e)\n    ```\n\n3.  **`GPOTrainer` and loss dispatching (src/alignment/gpo.py):**\n    ```python\n    class GPOTrainer(DPOTrainer):\n        def __init__(self, *args, func=lambda: None, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.func = func\n\n        def gpo_loss(\n            self,\n            policy_chosen_logps: torch.FloatTensor,\n            policy_rejected_logps: torch.FloatTensor,\n            reference_chosen_logps: torch.FloatTensor,\n            reference_rejected_logps: torch.FloatTensor,\n        ) -> torch.FloatTensor:\n            if self.loss_type == \"epo\": # 'epo' is the dynamic one from LLM\n                return self.func(\n                    self,\n                    policy_chosen_logps,\n                    policy_rejected_logps,\n                    reference_chosen_logps,\n                    reference_rejected_logps,\n                )\n            elif self.loss_type == \"aqfl\":\n                return self.adaptive_quantile_feedback_loss(\n                    policy_chosen_logps,\n                    policy_rejected_logps,\n                    reference_chosen_logps,\n                    reference_rejected_logps,\n                )\n            # ... (other loss types)\n            elif self.loss_type == \"lrml\": # Log Ratio Modulated Loss\n                return self.log_ratio_modulated_loss(\n                    policy_chosen_logps,\n                    policy_rejected_logps,\n                    reference_chosen_logps,\n                    reference_rejected_logps,\n                )\n            # ... (other loss types)\n            else:\n                raise ValueError(\n                    f\"Unknown loss type: {self.loss_type}. Should be one of ['sigmoid', 'hinge', 'ipo', 'kto_pair', 'bco_pair', 'epo']\"\n                )\n\n        # ... (other loss functions)\n\n        def log_ratio_modulated_loss(\n            self,\n            policy_chosen_logps: torch.FloatTensor,\n            policy_rejected_logps: torch.FloatTensor,\n            reference_chosen_logps: torch.FloatTensor,\n            reference_rejected_logps: torch.FloatTensor,\n        ) -> torch.FloatTensor:\n            tau = 0.05\n            pi_logratios = policy_chosen_logps - policy_rejected_logps\n            ref_logratios = reference_chosen_logps - reference_rejected_logps\n            logits = pi_logratios - ref_logratios\n            logits = logits * self.beta\n            # Modulate the mixing coefficient based on the log ratio magnitudes\n            log_ratio_modulation = torch.sigmoid(logits / tau)\n            logistic_component = -F.logsigmoid(logits)\n            exp_component = torch.exp(-logits)\n            # Blend between logistic and exponential component based on log ratio modulation\n            losses = logistic_component * (1 - log_ratio_modulation) + exp_component * log_ratio_modulation\n            return losses\n    ```\n\n4.  **Dynamic loss loading in `run_gpo.py` (scripts/run_gpo.py):**\n    ```python\n    def main():\n        # ... (parser and setup)\n        func = None\n\n        if training_args.loss_type == \"epo\":\n            with open(f\"recipes/zephyr-{size}-gemma/gpo/tests.json\", \"r\") as f:\n                info = json.load(f)\n            latest = info[-1]\n            code = latest[\"code\"]\n            print(f\"RUNNING: {latest['name']}\")\n            print(f\"CODE:\\n{code}\")\n            training_args.output_dir = f\"data/zephyr-{size}-gemma-{latest['name']}\"\n            # Namespace dictionary to hold the execution context\n            namespace = {}\n            # Execute the function definition string within the provided namespace\n            exec(code, globals(), namespace)\n            # Assert that there's only one function\n            names = list(namespace.keys())\n            assert len(names) == 1, f\"{len(names)} things in namespace\"\n            func = namespace[names[0]]\n            assert callable(func), f\"{func} is not callable\"\n\n        # ... (rest of the training setup)\n\n        trainer = GPOTrainer(\n            model,\n            ref_model,\n            # ...\n            loss_type=training_args.loss_type,\n            func=func,\n        )\n\n        # ... (training loop and save model)\n    ```",
    "Experiment Result": "The LLM-driven objective discovery pipeline employs GPT-4 (`gpt_model = \"gpt-4\"` in `scripts/launch_evo.py`) to iteratively propose and implement Python code for preference optimization loss functions.\n\n**Pipeline Steps and Settings:**\n1.  **Initialization**: The pipeline starts with established loss functions: DPO, HINGE (SLiC), IPO, and KTO. Their baseline MT-Bench scores are provided:\n    *   For 2B models: DPO: 4.75, HINGE: 4.72, IPO: 4.69, KTO: 4.68.\n    *   For 7B models: DPO: 7.8875, HINGE: 7.88125, IPO: 7.84, KTO: 7.603125.\n    (From `init_archive` in `scripts/launch_evo.py`)\n2.  **LLM Interaction**: GPT-4 is prompted with a system message guiding it to generate new loss functions in JSON format, including a 'thought' process, 'name', and 'code'. It receives feedback in the form of MT-Bench scores for its proposed functions.\n    (From `system_prompt` and main loop in `scripts/launch_evo.py`)\n3.  **Code Validation**: Each proposed loss function undergoes unit tests (`validate_code` in `scripts/launch_evo.py`). These tests verify:\n    *   The code defines exactly one callable function.\n    *   The function runs without errors with `torch.randn` inputs.\n    *   The output loss is a `torch.FloatTensor` of shape `(batch_size,)` (e.g., `(10,)`).\n    *   No NaNs are present in the calculated loss or its gradients.\n    *   A `beta=0.05` is used for validation purposes.\n4.  **LLM Fine-tuning (with proposed loss function)**:\n    *   **Models**: 2B or 7B parameter models (e.g., Zephyr-Gemma variants).\n    *   **Hardware**: Training is distributed using `accelerate launch` with a `deepspeed_zero3.yaml` configuration, utilizing 4 or 8 GPUs.\n    *   **Gradient Accumulation**: When using 4 GPUs, `gradient_accumulation_steps=16` is applied.\n    *   **Trainer**: The `GPOTrainer` (a subclass of `DPOTrainer`) is used, dynamically loading the Python code for the proposed loss function if `loss_type` is 'epo'.\n    *   **Data**: The `CarperAI/openai_summarize_comparisons` dataset (TL;DR summarization) and `ZHZisZZ/imdb_preference` (IMDb positive text generation) are used, preprocessed with chat templates and decontaminated from HumanEval samples.\n    *   **Hyperparameters**: A `beta` parameter is passed to the trainer (e.g., `training_args.beta`), which influences the scaling of logits within the loss function (e.g., `self.beta * logits`). Initial tests often use `beta=0.05`, but sweeps exploring `beta` values like `[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5]` were conducted.\n    (From `scripts/launch_evo.py`, `scripts/run_gpo.py`, `src/alignment/gpo.py`, `scripts/plotting/plot_beta_sweep.py`, `scripts/plotting/plot_losses.py`)\n5.  **Evaluation**: The fine-tuned LLM is evaluated on MT-Bench.\n    *   **Metric**: The average MT-Bench score across two turns is reported.\n    *   **Tools**: FastChat's LLM Judge (`../FastChat/fastchat/llm_judge`) is used for generating model answers and judgments (using GPT-4).\n    *   **Generation Parameters**: During evaluation for MT-Bench, `max_new_tokens` can be specified (e.g., 256 tokens in some contexts) and `num-choices` (number of generations for judgment, default 5) are used.\n    (From `evaluate_gpo` in `scripts/launch_evo.py`, `scripts/run_mt_bench.py`)\n6.  **Model Management**: Based on MT-Bench scores, models are managed:\n    *   Uploaded to Hugging Face Hub if score >= 7.9.\n    *   Deleted locally if score <= 7.75.\n    (From `evaluate_gpo` in `scripts/launch_evo.py`)\n\n**DiscoPOP (LRML) Specific Settings**: The best-performing discovered algorithm, Log Ratio Modulated Loss (LRML), includes an intrinsic `tau` parameter fixed at `0.05` (`tau = 0.05` in `log_ratio_modulated_loss` in `src/alignment/gpo.py`)."
}{
    "Title": "Language Generation with Strictly Proper Scoring Rules",
    "Main Contributions": "This paper addresses the limitations of maximum likelihood estimation (MLE) based on the logarithmic score in language generation, particularly its unbounded nature and sensitivity to perturbations. The main contribution is proposing a straightforward strategy to adapt non-local strictly proper scoring rules, such as the Brier score and Spherical score, for language generation by distributing them at the token level. The study also introduces 'score smoothing' to enable honest label smoothing for arbitrary scoring rules. Key findings indicate that simply substituting the loss function with alternative strictly proper scoring rules, especially during fine-tuning, can yield substantial improvements in generation capabilities for both smaller Transformer models and large language models (LLMs) like LLaMA-7B and LLaMA-13B. The research highlights the distinct learning dynamics of different strictly proper scores and suggests that a comprehensive evaluation of language models should consider multiple scores beyond perplexity.",
    "Methodology": "The core methodology involves adapting non-local strictly proper scoring rules to language generation. This is achieved by: 1. **Token-Level Scoring Rules**: Decomposing sequence probability autoregressively and applying scoring rules at the token level for conditional token prediction. This transforms intractable sequence-level calculations into feasible token-level ones. The Brier score (LBrier) and Spherical score (LSpherical) are implemented as token-level loss functions. 2. **Score Smoothing**: A general smoothing technique that extends label smoothing to arbitrary scoring rules, defined as Sϵ(p, i) = (1− ϵ) · S(p, i) + ϵ/m · ΣS(p, j). An 'enhanced masked logarithmic score' (Sϵ log) is introduced to strengthen the smoothing effect for under-smooth labels by applying the logarithmic score to probabilities below a certain threshold. The paper also explores the α-power score and pseudo-spherical score, which are more general forms of Brier and Spherical scores respectively, parametrized by α. Additionally, the study briefly investigates using modified Brier and Spherical scores as beam search objectives.",
    "Experimental Setup": "The method was evaluated on: 1. **Machine Translation**: WMT14 English-French (En-Fr) and English-German (En-De) using BPE with a 32K joint vocabulary, and the TED bilingual dataset. LLMs were evaluated on four WMT22 translation tasks (Chinese-to-English, English-to-Chinese, German-to-English, and English-to-German). Translation quality was measured by BLEU. 2. **Abstractive Summarization**: CNN/DailyMail, measured by ROUGE-1, ROUGE-2, and ROUGE-L. 3. **Large Language Models (LLMs)**: LLaMA-7B and LLaMA-13B were instruction-tuned using the Alpaca dataset (52K instruction-following demonstrations). LLM open-ended question answering capabilities were evaluated on MT-bench. Models include Transformer-base/big, and LLaMA models. Experiments involved training from scratch with different scoring rules, fine-tuning pre-trained models (with logarithmic score) using alternative scores, and instruction tuning LLMs with new loss functions. Hyperparameters were fixed for fine-tuning to isolate the effect of the loss function. Beam search was used for decoding, with specific beam sizes and length penalties.",
    "Limitations": "The paper acknowledges several limitations: 1. When training language generation models from scratch, alternative strictly proper scoring rules (Brier, Spherical) exhibit slower convergence and generally lower performance compared to the logarithmic score. This is hypothesized to be due to their distinct learning dynamics and the use of hyperparameters optimized for the logarithmic score. 2. The boundedness of Brier and Spherical scores means they impose a relatively mild penalty when score smoothing is ignored, potentially causing models to disregard the smoothing term, especially for the spherical score. This necessitated the introduction of the masked logarithmic score to enhance the smoothing effect. 3. In the exploratory analysis of beam search objectives, the logarithmic score generally yielded the best performance, indicating that alternative scores did not universally outperform it in this context. 4. For the more general α-power and pseudo-spherical scores, performance tended to decline when α > 2, suggesting that α=2 (the Brier/Spherical score case) represents a stable optimum rather than a universally improvable parameter.",
    "Future Research Directions": "The paper opens several avenues for future research: 1. Investigating if other strictly proper scoring rules exist that could yield superior performance during either pre-training or fine-tuning. 2. Delving deeper into the factors that contribute to the observed performance differences among strictly proper scoring rules to better understand their optimization trajectories and learning dynamics. 3. Developing new metrics or properties that can effectively predict a scoring rule's suitability for training language generation models. 4. Exploring the utility of these alternative scoring rules as evaluation metrics, akin to perplexity, for assessing the calibration of language generation models. 5. Conducting a more exhaustive investigation into applying different strictly proper scoring rules as objectives for beam search, to determine if any can consistently surpass the performance of the logarithmic score in this context.",
    "Experiment Code": null,
    "Experiment Result": null
}

# Instructions
Following the instructions below, please provide an evaluation of the new method.
Since I aim to pursue research of high academic significance, I request that the assessment be conducted with rigorous standards.
- output
    - novelty_reason
        - Determine whether the new method has novelty, and output the reason.
        - The reason should be as specific as possible.
        - Carefully review the content of the studies provided in "Related Works" before outputting.
    - novelty_score
        - Score the novelty of the new method on a scale of 1 to 10, where 1 means no novelty at all and 10 means extremely high novelty.
    - significance_reason
        - Determine whether the new method is significant, and output the reason.
        - Significance includes both academic and societal importance.
    - significance_score
        - Score the significance of the new method on a scale of 1 to 10, where 1 means no significance at all and 10 means extremely high significance.
Output:
{'novelty_reason': 'Among the papers supplied, the closest antecedent is “Language Generation with Strictly Proper Scoring Rules”, which already studies Brier and Spherical scores for LM pre-training / fine-tuning and explicitly reports the slow convergence and worse perplexity of Brier when training from scratch.  However, that work treats each score in isolation and, while it proposes “score smoothing”, it never investigates a time-varying or hybrid objective that starts as cross-entropy and gradually transitions to Brier.  The other related work, DiscoPOP, does mix two losses, but in a completely different RLHF (preference-pair) setting and with modulation based on example-level log-ratios, not a deterministic training-step schedule.  Therefore, the idea of a scheduled convex combination of CE and Brier tailored to address the early-vs-late gradient magnitude issue for token-level language modelling appears absent from the cited literature.  The proposal is conceptually simple but genuinely new in this specific context.', 'novelty_score': 6, 'significance_reason': 'Calibration directly affects the trustworthiness of LLM probability outputs (e.g., when used in chain-of-thought self-consistency, safety filters, or downstream uncertainty estimation).  Current practice largely sticks to pure cross-entropy because of training speed, sacrificing calibration, or to pure Brier during fine-tuning, sacrificing perplexity.  The Hybrid Log–Brier loss keeps implementation cost near zero (one extra line) and one intuitive hyper-parameter (switch point), yet in the preliminary experiment recovers ≈90 % of the calibration gain of Brier while giving up only ≈5 % in perplexity relative to CE.  If these findings scale, practitioners could adopt the method immediately in large-scale pre-training and fine-tuning pipelines, improving reliability without extra compute or architecture changes.  Academically, the work also provides a controlled test-bed for studying the optimisation dynamics of bounded versus unbounded proper scores.  Impact is therefore meaningful but not transformative.', 'significance_score': 6}
